{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ljppro/how-to-read-pytorch/blob/master/DTensor_Examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02DScykwY2Rz"
      },
      "source": [
        "# Install PyTorch nightly to try out DTensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUysr18vaeyW",
        "outputId": "51bfadb5-fdcc-4749-baf8-b9b8dcf73543"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting expecttest\n",
            "  Downloading expecttest-0.2.1-py3-none-any.whl (7.4 kB)\n",
            "Collecting hypothesis\n",
            "  Downloading hypothesis-6.100.0-py3-none-any.whl (458 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m458.0/458.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from hypothesis) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from hypothesis) (2.4.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from hypothesis) (1.2.0)\n",
            "Installing collected packages: hypothesis, expecttest\n",
            "Successfully installed expecttest-0.2.1 hypothesis-6.100.0\n"
          ]
        }
      ],
      "source": [
        "!pip install expecttest hypothesis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "# Define a function that raises a warning\n",
        "def example_function():\n",
        "    warnings.warn(\"This is a warning messadsasfadfge\")\n",
        "\n",
        "# Use a context manager to catch and print warnings\n",
        "with warnings.catch_warnings(record=True) as warning_list:\n",
        "    example_function()\n",
        "\n",
        "# Print the captured warnings\n",
        "for warning in warning_list:\n",
        "    print(f\"Captured Warning: {warning.message}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qcan4mTpNKRL",
        "outputId": "80def8ae-2e58-43fb-967f-158954eb2d42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Captured Warning: This is a warning messadsasfadfge\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUkAOw5UbYqg",
        "outputId": "3957399e-29c5-413d-8d1d-a5da1280ee0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6xam23UmzSr"
      },
      "source": [
        "## import some testing utils to run DTensor in notebook\n",
        "\n",
        "Since setup multiprocessing in a notebook is challenging (you can easily do it in `*.py` but with notebook there need to be some hacks, we developed some testing utils to spawn multiple threads and \"mimic\" the ProcessGroup communicator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQtDT6FZdnvH"
      },
      "outputs": [],
      "source": [
        "from torch.testing._internal.common_distributed import spawn_threads_and_init_comms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stYPKb9Beq4e"
      },
      "outputs": [],
      "source": [
        "WORLD_SIZE=4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "V1oJCSl2a6ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor([[1,2,3],[4,5,6]]).sum(dim=0)"
      ],
      "metadata": {
        "id": "gCulqupdb9aq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fd162b0-b141-4371-8be7-14d0d4775a6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([5, 7, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra5Evyu7mE8Y"
      },
      "source": [
        "# DTensor examples\n",
        "\n",
        "DTensor will prototype release in PyTorch 2.0, let's try out some examples in this notebook to play around with DTensor.\n",
        "\n",
        "First we need some necessary imports for DTensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGRdVBGxo1Fb"
      },
      "outputs": [],
      "source": [
        "# some necessary imports\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch.distributed._tensor import DTensor, DeviceMesh, Shard, Replicate, distribute_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW6x1YpsHkPq"
      },
      "source": [
        "How we could shard a big tensor across ranks?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpOQz5jTmJEK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2569b798-7db5-4199-8b6c-3c49879a1791"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "on rank: 0, dtensor global shape: torch.Size([653, 10]), local shape: torch.Size([164, 10])\n",
            "\n",
            "   global device: meta, local device: meta\n",
            "\n",
            "on rank: 2, dtensor global shape: torch.Size([653, 10]), local shape: torch.Size([164, 10])\n",
            "\n",
            "on rank: 1, dtensor global shape: torch.Size([653, 10]), local shape: torch.Size([164, 10])\n",
            "\n",
            "on rank: 3, dtensor global shape: torch.Size([653, 10]), local shape: torch.Size([161, 10])\n",
            "\n",
            "   global device: meta, local device: meta\n",
            "   global device: meta, local device: meta\n",
            "\n",
            "\n",
            "   global device: meta, local device: meta\n",
            "\n"
          ]
        }
      ],
      "source": [
        "@spawn_threads_and_init_comms\n",
        "def shard_big_tensor(world_size):\n",
        "  mesh = DeviceMesh(\"cpu\", [0, 1, 2, 3])\n",
        "  big_tensor = torch.randn((653, 10), device=\"meta\")\n",
        "  dtensor = distribute_tensor(big_tensor, mesh, [Shard(0)])\n",
        "  print(f\"on rank: {dist.get_rank()}, dtensor global shape: {dtensor.shape}, local shape: {dtensor.to_local().shape}\\n\")\n",
        "  print(f\"   global device: {dtensor.device}, local device: {dtensor.to_local().device}\\n\")\n",
        "\n",
        "shard_big_tensor(WORLD_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75wopAPiHndr"
      },
      "source": [
        "What if we want to replicate a big tensor across ranks?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMruZYQxqHHq",
        "outputId": "cee5b362-eba1-4676-ccde-27af61714b8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "on rank: 1, dtensor global shape: torch.Size([888, 10]), local shape: torch.Size([888, 10])\n",
            "\n",
            "on rank: 2, dtensor global shape: torch.Size([888, 10]), local shape: torch.Size([888, 10])\n",
            "\n",
            "on rank: 0, dtensor global shape: torch.Size([888, 10]), local shape: torch.Size([888, 10])\n",
            "\n",
            "on rank: 3, dtensor global shape: torch.Size([888, 10]), local shape: torch.Size([888, 10])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "@spawn_threads_and_init_comms\n",
        "def replicate_big_tensor(world_size):\n",
        "  mesh = DeviceMesh(\"cpu\", [0, 1, 2, 3])\n",
        "  big_tensor = torch.randn((888, 10))\n",
        "  dtensor = distribute_tensor(big_tensor, mesh, [Replicate()])\n",
        "  print(f\"on rank: {dist.get_rank()}, dtensor global shape: {dtensor.shape}, local shape: {dtensor.to_local().shape}\\n\")\n",
        "\n",
        "replicate_big_tensor(WORLD_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vgoa6kCHrUM"
      },
      "source": [
        "What if we want to do some more complex sharding placements, say we want to shard this big tensor in a subset of devices, and replicate the shards in another shard of devices?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edxxyyTD2KCZ",
        "outputId": "c800afba-be98-4ba8-dc2b-e2c27bfc969f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeviceMesh([[0, 1], [2, 3]])\n",
            "DeviceMesh([[0, 1], [2, 3]])\n",
            "\n",
            "\n",
            "DeviceMesh([[0, 1], [2, 3]])\n",
            "\n",
            "DeviceMesh([[0, 1], [2, 3]])\n",
            "\n",
            "on rank: 3 === DTensor(local_tensor=-68.71055603027344, device_mesh=DeviceMesh([[0, 1], [2, 3]]), placements=(Replicate(), _Partial(reduce_op=RedOpType.SUM)))\n",
            "\n",
            "on rank: 3, dtensor global shape: torch.Size([888, 10]), local shape: torch.Size([888, 5])\n",
            "\n",
            "on rank: 1 === DTensor(local_tensor=-68.71055603027344, device_mesh=DeviceMesh([[0, 1], [2, 3]]), placements=(Replicate(), _Partial(reduce_op=RedOpType.SUM)))\n",
            "\n",
            "on rank: 0 === DTensor(local_tensor=-19.24100112915039, device_mesh=DeviceMesh([[0, 1], [2, 3]]), placements=(Replicate(), _Partial(reduce_op=RedOpType.SUM)))\n",
            "\n",
            "on rank: 0, dtensor global shape: torch.Size([888, 10]), local shape: torch.Size([888, 5])\n",
            "\n",
            "on rank: 1, dtensor global shape: torch.Size([888, 10]), local shape: torch.Size([888, 5])\n",
            "on rank: 2 === DTensor(local_tensor=-19.24100112915039, device_mesh=DeviceMesh([[0, 1], [2, 3]]), placements=(Replicate(), _Partial(reduce_op=RedOpType.SUM)))\n",
            "\n",
            "\n",
            "on rank: 2, dtensor global shape: torch.Size([888, 10]), local shape: torch.Size([888, 5])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "@spawn_threads_and_init_comms\n",
        "def partially_shard_tensor(world_size):\n",
        "  # if we want to distributed a tensor with both replication and sharding\n",
        "  # create a 2-d mesh\n",
        "  device_mesh = DeviceMesh(\"cpu\", torch.arange(world_size).reshape(2, 2))\n",
        "  print(str(device_mesh) + \"\\n\")\n",
        "\n",
        "  big_tensor = torch.randn((888, 10))\n",
        "  # replicate across the first dimension of device mesh, then sharding (on tensor dim 0) on the second dimension of device mesh\n",
        "  spec=[Replicate(), Shard(1)]\n",
        "  partial_shard = distribute_tensor(big_tensor, device_mesh=device_mesh, placements=spec)\n",
        "  print(f\"on rank: {dist.get_rank()} === {partial_shard.sum()}\\n\")\n",
        "  print(f\"on rank: {dist.get_rank()}, dtensor global shape: {partial_shard.shape}, local shape: {partial_shard.to_local().shape}\\n\")\n",
        "\n",
        "\n",
        "partially_shard_tensor(WORLD_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23qKr1HmI_G7"
      },
      "source": [
        "How does DTensor intereacts with torch.Tensor?\n",
        "\n",
        "We offer two APIs to convert from/to torch.Tensor:\n",
        "- `from_local`, where it converts a torch.Tensor to a DTensor in SPMD fashion\n",
        "- `to_local`, where we convert the DTensor to a torch.Tensor on each rank in SPMD fashion.\n",
        "\n",
        "Note that both `from_local` and `to_local` are differentiable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmrIJENuHcwj",
        "outputId": "915658ef-6786-438f-d85e-d590a529d9f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "on rank: 0, dtensor global shape: torch.Size([32, 8]), local shape: torch.Size([8, 8])\n",
            "on rank: 1, dtensor global shape: torch.Size([32, 8]), local shape: torch.Size([8, 8])\n",
            "on rank: 3, dtensor global shape: torch.Size([32, 8]), local shape: torch.Size([8, 8])\n",
            "on rank: 2, dtensor global shape: torch.Size([32, 8]), local shape: torch.Size([8, 8])\n"
          ]
        }
      ],
      "source": [
        "@spawn_threads_and_init_comms\n",
        "def dtensor_from_local_to_local(world_size):\n",
        "  mesh = DeviceMesh(\"cpu\", torch.arange(world_size))\n",
        "  # create a DistributedTensor that shards on dim 0, from a local torch.Tensor\n",
        "  local_tensor = torch.randn((8, 8), requires_grad=True)\n",
        "  rowwise_placement = [Shard(0)]\n",
        "  rowwise_tensor = DTensor.from_local(local_tensor, mesh, rowwise_placement)\n",
        "  print(f\"on rank: {dist.get_rank()}, dtensor global shape: {rowwise_tensor.shape}, local shape: {rowwise_tensor.to_local().shape}\")\n",
        "\n",
        "dtensor_from_local_to_local(WORLD_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D72r2bk2kqaU"
      },
      "source": [
        "What if we want to change the layout of the DTensor (i.e. we want to convert a row-wise sharding DTensor to col-wise sharding, or we want to convert it back to a replicated DTensor? DTensor offers a `redistribute` API to automatically do the transformation:\n",
        "\n",
        "- `dtensor.redistribute(mesh: DeviceMesh, placements: Sequence[Placement])`\n",
        "\n",
        "Let's see an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOKIaL-8JcgB",
        "outputId": "cb813d2f-0d44-4755-9f0a-2b20a46402b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "on rank: 0, col-wise dtensor global shape: torch.Size([32, 8]), local shape: torch.Size([32, 2])\n",
            "on rank: 1, col-wise dtensor global shape: torch.Size([32, 8]), local shape: torch.Size([32, 2])\n",
            "on rank: 3, col-wise dtensor global shape: torch.Size([32, 8]), local shape: torch.Size([32, 2])on rank: 2, col-wise dtensor global shape: torch.Size([32, 8]), local shape: torch.Size([32, 2])\n",
            "\n",
            "on rank: 0, replicate dtensor global shape: torch.Size([32, 8]), local shape: torch.Size([32, 8])on rank: 1, replicate dtensor global shape: torch.Size([32, 8]), local shape: torch.Size([32, 8])on rank: 2, replicate dtensor global shape: torch.Size([32, 8]), local shape: torch.Size([32, 8])\n",
            "\n",
            "\n",
            "on rank: 3, replicate dtensor global shape: torch.Size([32, 8]), local shape: torch.Size([32, 8])\n"
          ]
        }
      ],
      "source": [
        "@spawn_threads_and_init_comms\n",
        "def dtensor_reshard(world_size):\n",
        "  mesh = DeviceMesh(\"cpu\", torch.arange(world_size))\n",
        "  rowwise_placement = [Shard(0)]\n",
        "  colwise_placement = [Shard(1)]\n",
        "  # create a rowwise tensor\n",
        "  local_tensor = torch.randn(8, 8)\n",
        "  rowwise_tensor = DTensor.from_local(local_tensor, mesh, rowwise_placement)\n",
        "  # reshard the current row-wise tensor to a colwise tensor or replicate tensor\n",
        "  replica_placement = [Replicate()]\n",
        "  colwise_tensor = rowwise_tensor.redistribute(mesh, colwise_placement)\n",
        "  print(f\"on rank: {dist.get_rank()}, col-wise dtensor global shape: {colwise_tensor.shape}, local shape: {colwise_tensor.to_local().shape}\")\n",
        "  replica_tensor = colwise_tensor.redistribute(mesh, replica_placement)\n",
        "  print(f\"on rank: {dist.get_rank()}, replicate dtensor global shape: {replica_tensor.shape}, local shape: {replica_tensor.to_local().shape}\")\n",
        "\n",
        "dtensor_reshard(WORLD_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZpLNilOqZgl"
      },
      "source": [
        "# Tensor Parallel Examples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TNNw52KSrg5"
      },
      "source": [
        "Below we presented an example for Tensor Parallel (TP) and we first defined a dummy model which is essentially a two-layer multilayer perceptron (MLP)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QP_zNr-Iqp_u"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.distributed._tensor import (\n",
        "    DeviceMesh,\n",
        ")\n",
        "from torch.distributed.tensor.parallel import (\n",
        "    PairwiseParallel,\n",
        "    parallelize_module,\n",
        ")\n",
        "\n",
        "ITER_TIME = 20\n",
        "\n",
        "class ToyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ToyModel, self).__init__()\n",
        "        self.net1 = nn.Linear(10, 32)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.net2 = nn.Linear(32, 5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net2(self.relu(self.net1(x)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mvv43kXRSXpi"
      },
      "source": [
        "We then create an example to show an E2E working flow from forward,\n",
        "backward and optimization.\n",
        "\n",
        "More context about API designs can be found in the [design](https://github.com/pytorch/pytorch/issues/89884). And it is built on top of Distributed Tensor shown above.\n",
        "\n",
        "We use the example of two `nn.Linear` layers with an element-wise `nn.RELU`\n",
        "in between to show an example of Megatron-LM, which was proposed in [paper](https://arxiv.org/abs/1909.08053).\n",
        "\n",
        "The basic idea is that we parallelize the first linear layer by column\n",
        "and also parallelize the second linear layer by row so that we only need\n",
        "one all reduce in the end of the second linear layer.\n",
        "\n",
        "We can speed up the model training by avoiding communications between\n",
        "two layers.\n",
        "\n",
        "To parallelize a nn module, we need to specify what parallel style we want\n",
        "to use and our `parallelize_module` API will parse and parallelize the modules\n",
        "based on the given `ParallelStyle`. We are using this PyTorch native Tensor\n",
        "Parallelism APIs in this example to show users how to use them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-cA98H3rKTO",
        "outputId": "4bb8eec6-e5ab-464a-f73c-003cbd774c12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create a sharding plan based on the given world_size\n",
            "Parallelize the module based on the given Parallel Style\n",
            "FWD Step: iter 0\n",
            "BWD Step: iter 0\n",
            "Optimization Step: iter 0\n",
            "FWD Step: iter 1\n",
            "BWD Step: iter 1\n",
            "Optimization Step: iter 1\n",
            "FWD Step: iter 2\n",
            "BWD Step: iter 2\n",
            "Optimization Step: iter 2\n",
            "FWD Step: iter 3\n",
            "BWD Step: iter 3\n",
            "Optimization Step: iter 3\n",
            "FWD Step: iter 4\n",
            "BWD Step: iter 4\n",
            "Optimization Step: iter 4\n",
            "FWD Step: iter 5\n",
            "BWD Step: iter 5\n",
            "Optimization Step: iter 5\n",
            "FWD Step: iter 6\n",
            "BWD Step: iter 6\n",
            "Optimization Step: iter 6\n",
            "FWD Step: iter 7\n",
            "BWD Step: iter 7\n",
            "Optimization Step: iter 7\n",
            "FWD Step: iter 8\n",
            "BWD Step: iter 8\n",
            "Optimization Step: iter 8\n",
            "FWD Step: iter 9\n",
            "BWD Step: iter 9\n",
            "Optimization Step: iter 9\n",
            "FWD Step: iter 10\n",
            "BWD Step: iter 10\n",
            "Optimization Step: iter 10\n",
            "FWD Step: iter 11\n",
            "BWD Step: iter 11\n",
            "Optimization Step: iter 11\n",
            "FWD Step: iter 12\n",
            "BWD Step: iter 12\n",
            "Optimization Step: iter 12\n",
            "FWD Step: iter 13\n",
            "BWD Step: iter 13\n",
            "Optimization Step: iter 13\n",
            "FWD Step: iter 14\n",
            "BWD Step: iter 14\n",
            "Optimization Step: iter 14\n",
            "FWD Step: iter 15\n",
            "BWD Step: iter 15\n",
            "Optimization Step: iter 15\n",
            "FWD Step: iter 16\n",
            "BWD Step: iter 16\n",
            "Optimization Step: iter 16\n",
            "FWD Step: iter 17\n",
            "BWD Step: iter 17\n",
            "Optimization Step: iter 17\n",
            "FWD Step: iter 18\n",
            "BWD Step: iter 18\n",
            "Optimization Step: iter 18\n",
            "FWD Step: iter 19\n",
            "BWD Step: iter 19\n",
            "Optimization Step: iter 19\n",
            "Training finished\n"
          ]
        }
      ],
      "source": [
        "def print0(msg, rank):\n",
        "    if rank == 0:\n",
        "        print(msg)\n",
        "\n",
        "@spawn_threads_and_init_comms\n",
        "def demo_tp(world_size):\n",
        "    \"\"\"\n",
        "    Main body of the demo of a basic version of tensor parallel by using\n",
        "    PyTorch native APIs.\n",
        "    \"\"\"\n",
        "    rank = dist.get_rank()\n",
        "    print0(\"Create a sharding plan based on the given world_size\", rank)\n",
        "    # create a sharding plan based on the given world_size.\n",
        "    device_mesh = DeviceMesh(\n",
        "        \"cpu\",\n",
        "        torch.arange(world_size),\n",
        "    )\n",
        "\n",
        "    # create model and move it to GPU with id rank\n",
        "    model = ToyModel()\n",
        "    # Create a optimizer for the parallelized module.\n",
        "    LR = 0.25\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "    print0(\"Parallelize the module based on the given Parallel Style\", rank)\n",
        "    # Parallelize the module based on the given Parallel Style.\n",
        "    model = parallelize_module(model, device_mesh, PairwiseParallel())\n",
        "\n",
        "    # Perform a num of iterations of forward/backward\n",
        "    # and optimizations for the sharded module.\n",
        "    for i in range(ITER_TIME):\n",
        "        inp = torch.rand(20, 10)\n",
        "        output = model(inp)\n",
        "        print0(f\"FWD Step: iter {i}\", rank)\n",
        "        output.sum().backward()\n",
        "        print0(f\"BWD Step: iter {i}\", rank)\n",
        "        optimizer.step()\n",
        "        print0(f\"Optimization Step: iter {i}\", rank)\n",
        "\n",
        "    print0(\"Training finished\", rank)\n",
        "\n",
        "demo_tp(WORLD_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng_-kEnCGU9Z"
      },
      "source": [
        "# 2D parallel and beyond\n",
        "\n",
        "For 2D parallel with FullyShardedDataParallel(FSDP), since FSDP can only run on GPU now, we attached a [link](https://github.com/pytorch/pytorch/blob/master/test/distributed/tensor/parallel/test_2d_parallel.py) here as a reference.\n",
        "\n",
        "And per community's ask for combining TP with PyTorch native pipeline parallel, aka, [PiPPy](https://github.com/pytorch/tau/tree/main). We also provided an [link](https://github.com/pytorch/tau/blob/main/examples/tp%2Bpp/pippy_tp.py) to the example showing how TP works with PiPPy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkIB2jGXc-HP"
      },
      "source": [
        "# Call for Actions\n",
        "\n",
        "Both DTensor and Tensor Parallel are in early stage of development (prototype release along with PyTorch 2.0). Feel free try it out now with the nightly, or with the upcoming 2.0 release.\n",
        "\n",
        "If you meet some blockers, feel free file an github issue, or open a PR to contribute!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}